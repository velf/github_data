{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_pullrequest(event):\n",
    "    if event.get('repo') and event['actor'].get('login'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repo']['id']\n",
    "        user=event['actor']['login']\n",
    "        if event['payload']['action']=='closed':\n",
    "            closed_at=date_from_json(event['created_at'])\n",
    "        else:\n",
    "            closed_at=''\n",
    "            merged_at=''\n",
    "        if event['payload']['action']=='opened':\n",
    "            opened_at=date_from_json(event['created_at'])\n",
    "            merged_at=''\n",
    "        else:\n",
    "            opened_at=''\n",
    "            merged_at=''\n",
    "        return (pullreq_id, repoid, user, opened_at, closed_at, merged_at)\n",
    "    elif event.get('repository') and event['payload'].get('actor'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['payload']['actor']\n",
    "        opened_at=date_from_json(event.get('created_at'))\n",
    "        if event['payload']['action']=='closed':\n",
    "            closed_at=date_from_json(event['payload']['pull_request'].get('closed_at'))\n",
    "        else:\n",
    "            closed_at=''\n",
    "        if event['payload']['pull_request']['merged_at']==True:\n",
    "            merged_at=date_from_json(event['payload']['pull_request'].get('merged_at'))\n",
    "        else:\n",
    "            merged_at=''\n",
    "        return (pullreq_id, repoid, user, opened_at, closed_at, merged_at)\n",
    "    elif event.get('repository'):\n",
    "        pullreq_id=event['payload']['pull_request']['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['actor']\n",
    "        opened_at=date_from_json(event.get('created_at'))\n",
    "        if event['payload']['action']=='closed':\n",
    "            closed_at=date_from_json(event['payload']['pull_request'].get('closed_at'))\n",
    "        else:\n",
    "            closed_at='' \n",
    "        if event['payload']['pull_request']['merged_at']==True:\n",
    "            merged_at=date_from_json(event['payload']['pull_request'].get('merged_at'))\n",
    "        else:\n",
    "            merged_at=''\n",
    "        return (pullreq_id, repoid, user, opened_at, closed_at, merged_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pprint\n",
    "import json\n",
    "import sqlite3\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#dbpath = '/Users/mszell/data/githubarchive/derived/'\n",
    "#dbname = 'githubarchive.db'\n",
    "#datapath = '/Users/mszell/data/githubarchive/raw/'\n",
    "\n",
    "dbpath = ''\n",
    "dbname = 'REPO_TEST4.sqlite'\n",
    "datapath = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_from_filename(fname):\n",
    "    dt = datetime.datetime.strptime(fname[len('githubarchive_'):-len('.json')], '%Y-%m-%d')\n",
    "    return dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_from_json(creation_date):\n",
    "    dt = datetime.datetime.strptime(creation_date[:19], '%Y-%m-%dT%H:%M:%S')\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parse all data from githubarchive (daily files) to build a new sqlite db\\n\\nThe database contains the following tables:\\nrepository: (repositoryid INTEGER PRIMARY KEY, repositoryname TEXT, userid TEXT, created_at INTEGER, language TEXT, description TEXT, url TEXT, fork INTEGER)\\npush: (repositoryid INTEGER PRIMARY KEY, pushed_at DATE, userid TEXT)\\nwatch: (repositoryid INTEGER PRIMARY KEY, watched_at DATE,  userid TEXT, watchers INTEGER)\\nmemberadd: (repositoryid INTEGER PRIMARY KEY, added_at DATE, repositoryid INTEGER, userid INTEGER, byuserid INTEGER)\\nfollow: (repositoryid INTEGER PRIMARY KEY, followed_at DATE, targetuserid TEXT, followers INTEGER, repos  byuserid TEXT)\\nstar:()\\n\\nThe tables contain data relevant only to those repositories which have been created in the timespan that the files cover. Exception is table follows which contains all follow events.\\n\\nNote: This script will only work with githubarchive data starting at 2012-09-22. Reason: Before this time, the repositoryid does not exist for PushEvents.\\n\\n\\nParameters\\n----------\\nsee below\\ndbpath: string\\ndatapath: string\\n\\nDependencies\\n------------\\nsee imports below\\n\\nReturns\\n-------\\nn/a\\n\\nExample\\n-------\\n>>> python build_all.py\\n\\nSee Also\\n--------\\n\\n\\nCreated on  2016-10-27\\nLast update 2016-10-27\\n'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"Parse all data from githubarchive (daily files) to build a new sqlite db\n",
    "\n",
    "The database contains the following tables:\n",
    "repository: (repositoryid INTEGER PRIMARY KEY, repositoryname TEXT, userid TEXT, created_at INTEGER, language TEXT, description TEXT, url TEXT, fork INTEGER)\n",
    "push: (repositoryid INTEGER PRIMARY KEY, pushed_at DATE, userid TEXT)\n",
    "watch: (repositoryid INTEGER PRIMARY KEY, watched_at DATE,  userid TEXT, watchers INTEGER)\n",
    "memberadd: (repositoryid INTEGER PRIMARY KEY, added_at DATE, repositoryid INTEGER, userid INTEGER, byuserid INTEGER)\n",
    "follow: (repositoryid INTEGER PRIMARY KEY, followed_at DATE, targetuserid TEXT, followers INTEGER, repos  byuserid TEXT)\n",
    "star:()\n",
    "\n",
    "The tables contain data relevant only to those repositories which have been created in the timespan that the files cover. Exception is table follows which contains all follow events.\n",
    "\n",
    "Note: This script will only work with githubarchive data starting at 2012-09-22. Reason: Before this time, the repositoryid does not exist for PushEvents.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "see below\n",
    "dbpath: string\n",
    "datapath: string\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "see imports below\n",
    "\n",
    "Returns\n",
    "-------\n",
    "n/a\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> python build_all.py\n",
    "\n",
    "See Also\n",
    "--------\n",
    "\n",
    "\n",
    "Created on  2016-10-27\n",
    "Last update 2016-10-27\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #build_repository()\n",
    "    #build_push()\n",
    "    #build_watch()\n",
    "    #build_pull_request_opened()\n",
    "    #build_pull_request_closed()\n",
    "    #build_pull_request_merged()\n",
    "    #build_fork()\n",
    "    #build_star()\n",
    "    #build_memberadd()\n",
    "    #build_membership()\n",
    "    #build_follow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_create_event(event):\n",
    "    if event['payload'].get('object')=='repository' and event['repo'].get('id') and event['payload'].get('name') and event.get('actor').get('login'):\n",
    "        repo_name=event['payload'].get('name')\n",
    "        repoid=event['repo']['id']\n",
    "        created_at=date_from_json(event.get('created_at'))\n",
    "        user=event['actor']['login']\n",
    "        description='NA'\n",
    "        #print repoid, repo_name, user, created_at, description\n",
    "        return repoid, repo_name, user, created_at, description\n",
    "    elif event.get('payload') and event['payload'].get('ref_type')=='repository' and event.get('repository') and event.get('actor'):\n",
    "        repo_name=event['repository']['name']\n",
    "        repoid=event['repository']['id']\n",
    "        created_at=date_from_json(event['repository'].get('created_at'))\n",
    "        user=event['actor']\n",
    "        description=event['payload'].get('description')\n",
    "        return repoid, repo_name, user, created_at, description\n",
    "    elif event.get('payload') and event['payload'].get('ref_type')=='repository' and event.get('repo') and event.get('actor'):\n",
    "        repo_name=event['repo']['name']\n",
    "        repoid=event['repo']['id']\n",
    "        created_at=date_from_json(event.get('created_at'))\n",
    "        user=event['actor']['login']\n",
    "        description=event['payload'].get('description')\n",
    "        return repoid, repo_name, user, created_at, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(event):\n",
    "    #print event['type']\n",
    "    if type(event)==dict:\n",
    "        if event.get('repo') and event['repo'].get('id'):\n",
    "            d=date_from_json(event.get('created_at'))\n",
    "            if event.get('user'):\n",
    "                if event['user']['type']=='User':\n",
    "                    u=event['user']['login']\n",
    "                    p=event['repo']['id']\n",
    "                    return (u, p, d)\n",
    "            elif event.get('actor'):\n",
    "                if 'login' in event['actor'].keys():\n",
    "                    u=event['actor']['login']\n",
    "                    p=event['repo']['id']\n",
    "                    return (u, p, d)\n",
    "                elif event.get('payload') and event['payload'].get('actor'):\n",
    "                    u=event['payload']['actor']\n",
    "                    p=event['repo']['id']\n",
    "                    return (u, p, d)\n",
    "        elif event.get('repository'):\n",
    "            u=event['actor']\n",
    "            p=event['repository']['id']\n",
    "            d=date_from_json(event.get('created_at'))\n",
    "            return (u, p, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_pullrequest_opened(event):\n",
    "    if event.get('repo') and event['actor'].get('login'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repo']['id']\n",
    "        user=event['actor']['login']\n",
    "        if event['payload']['action']=='opened':\n",
    "            opened_at=date_from_json(event['created_at'])\n",
    "        return (pullreq_id, repoid, user, opened_at)\n",
    "    elif event.get('repository') and event['payload'].get('actor'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['payload']['actor']\n",
    "        opened_at=date_from_json(event.get('created_at'))\n",
    "        return (pullreq_id, repoid, user, opened_at)\n",
    "    elif event.get('repository'):\n",
    "        pullreq_id=event['payload']['pull_request']['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['actor']\n",
    "        opened_at=date_from_json(event.get('created_at'))\n",
    "        return (pullreq_id, repoid, user, opened_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_pullrequest_closed(event):\n",
    "    if event.get('repo') and event['actor'].get('login'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repo']['id']\n",
    "        user=event['actor']['login']\n",
    "        closed_at=date_from_json(event['created_at'])\n",
    "        return (pullreq_id, repoid, user, closed_at)\n",
    "    elif event.get('repository') and event['payload'].get('actor'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['payload']['actor']\n",
    "        closed_at=date_from_json(event['payload']['pull_request'].get('closed_at'))\n",
    "        return (pullreq_id, repoid, user, closed_at)\n",
    "    elif event.get('repository'):\n",
    "        pullreq_id=event['payload']['pull_request']['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['actor']\n",
    "        closed_at=date_from_json(event['payload']['pull_request'].get('closed_at'))\n",
    "        return (pullreq_id, repoid, user, closed_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_pullrequest_merged(event):\n",
    "    if event.get('repository') and event['payload'].get('actor'):\n",
    "        pullreq_id=event['id']\n",
    "        repoid=event['repository']['id']\n",
    "        user=event['payload']['actor']\n",
    "        merged_at=date_from_json(event['payload']['pull_request'].get('closed_at'))\n",
    "        #print pullreq_id, repoid, user, merged_at\n",
    "        return (pullreq_id, repoid, user, merged_at)\n",
    "    elif event.get('repo'):\n",
    "        pullreq_id=event['payload']['pull_request']['id']\n",
    "        repoid=event['repo']['id']\n",
    "        user=event['actor']['login']\n",
    "        merged_at=date_from_json(event['payload']['pull_request'].get('closed_at'))\n",
    "        #print pullreq_id, repoid, user, merged_at\n",
    "        return (pullreq_id, repoid, user, merged_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_repository():\n",
    "    # Create table\n",
    "    conn = sqlite3.connect(dbpath + dbname)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE IF EXISTS repository''')\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS repository\n",
    "                 (repositoryid INTEGER PRIMARY KEY, repositoryname TEXT, userid TEXT, created_at DATE, description TEXT)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    # Parse files\n",
    "    #filenames = [ f for f in listdir(datapath) if isfile(join(datapath,f)) ]\n",
    "    filenames=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_\") and i.endswith('json')])\n",
    "    for filename in filenames:\n",
    "        conn = sqlite3.connect(dbpath + dbname)\n",
    "        c = conn.cursor()\n",
    "        i = 0\n",
    "        with open(filename) as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                try: \n",
    "                    jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "                    #print jsonline.keys()\n",
    "                    if jsonline.get(\"type\") == \"CreateEvent\" and jsonline.get('payload').get('object')=='repository':\n",
    "                        try:\n",
    "                            repoid, repo_name, user, created_at, description=parse_create_event(jsonline)\n",
    "                            query = 'INSERT OR REPLACE INTO repository VALUES (?,?,?,?,?)'\n",
    "                            c.execute(query, (repoid, repo_name, user, created_at, description))\n",
    "                            i = i+1\n",
    "                            #print i\n",
    "                        except: # Rarely, JSON is not valid even if I tryy super hard\n",
    "                            pass\n",
    "                    elif jsonline.get(\"type\") == \"CreateEvent\" and jsonline.get('payload').get('ref_type')=='repository':\n",
    "                        repoid, repo_name, user, created_at, description=parse_create_event(jsonline)\n",
    "                        query = 'INSERT OR REPLACE INTO repository VALUES (?,?,?,?,?)'\n",
    "                        c.execute(query, (repoid, repo_name, user, created_at, description))\n",
    "                        i = i+1\n",
    "                except:\n",
    "                    for k in ['{\"repo\"'+l for l in line.split('{\"repo\"')[1:]]:\n",
    "                        jsonline=json.loads(unicode(k, errors='ignore').strip())\n",
    "                        last_event=jsonline\n",
    "                        if jsonline.get(\"type\") == \"CreateEvent\" and jsonline.get('payload').get('object')=='repository':\n",
    "                            try:\n",
    "                                repoid, repo_name, user, created_at, description=parse_create_event(jsonline)\n",
    "                                query = 'INSERT OR REPLACE INTO repository VALUES (?,?,?,?,?)'\n",
    "                                c.execute(query, (repoid, repo_name, user, created_at, description))\n",
    "                                i = i+1\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif jsonline.get(\"type\") == \"CreateEvent\" and jsonline['payload'].get('ref_type')=='repository':\n",
    "                            repoid, repo_name, user, created_at, description=parse_create_event(jsonline)\n",
    "                            query = 'INSERT OR REPLACE INTO repository VALUES (?,?,?,?,?)'\n",
    "                            c.execute(query, (repoid, repo_name, user, created_at, description))\n",
    "                            i = i+1\n",
    "            print \"Done file \" + filename + \". Found and inserted \" + str(i) + \" valid CreationEvents.\"\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_watch():\n",
    "    # Create table\n",
    "    conn = sqlite3.connect(dbpath + dbname)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE IF EXISTS watch''')\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS watch\n",
    "                 (repositoryid INTEGER, watched_at DATE, byuserlogin TEXT)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Parse files\n",
    "    #filenames = [ f for f in listdir(datapath) if isfile(join(datapath,f)) ]\n",
    "    filenames=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_\") and i.endswith('json')])\n",
    "    for filename in filenames:\n",
    "        conn = sqlite3.connect(dbpath + dbname)\n",
    "        c = conn.cursor()\n",
    "        i = 0\n",
    "        with open(filename) as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                try: \n",
    "                    jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "                    #print jsonline.keys()\n",
    "                    if jsonline.get(\"type\") == \"WatchEvent\":\n",
    "                        try:\n",
    "                            user, project, date=parse(jsonline)\n",
    "                            query = 'INSERT OR REPLACE INTO watch VALUES (?,?,?)'\n",
    "                            c.execute(query, (project, date, user))\n",
    "                            i = i+1\n",
    "                        except: #repoid or user id missing\n",
    "                            pass\n",
    "                except:\n",
    "                    for k in ['{\"repo\"'+l for l in line.split('{\"repo\"')[1:]]:\n",
    "                        jsonline=json.loads(unicode(k, errors='ignore').strip())\n",
    "                        last_event=jsonline\n",
    "                        if jsonline.get(\"type\") == \"WatchEvent\":\n",
    "                            try:\n",
    "                                user, project, date=parse(jsonline)\n",
    "                                query = 'INSERT OR REPLACE INTO watch VALUES (?,?,?)'\n",
    "                                c.execute(query, (project, date, user))\n",
    "                                i = i+1\n",
    "                            except: #repoid or user id missing\n",
    "                                pass\n",
    "        print \"Done file \" + filename + \". Found and inserted \" + str(i) + \" valid WatchEvents.\"\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_push():\n",
    "    # Create table\n",
    "    conn = sqlite3.connect(dbpath + dbname)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE IF EXISTS push''')\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS push\n",
    "                 (repositoryid INTEGER, pushed_at DATE, userlogin TEXT)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Parse files\n",
    "    #filenames = [ f for f in listdir(datapath) if isfile(join(datapath,f)) ]\n",
    "    filenames=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_\") and i.endswith('json')])\n",
    "    for filename in filenames:\n",
    "        conn = sqlite3.connect(dbpath + dbname)\n",
    "        c = conn.cursor()\n",
    "        i = 0\n",
    "        with open(filename) as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                try: \n",
    "                    jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "                    #print jsonline.keys()\n",
    "                    if jsonline.get(\"type\") == \"PushEvent\":\n",
    "                        try:\n",
    "                            user, project, date=parse(jsonline)\n",
    "                            query = 'INSERT OR REPLACE INTO push VALUES (?,?,?)'\n",
    "                            c.execute(query, (project, date, user))\n",
    "                            i = i+1\n",
    "                        except: #repoid or user id missing\n",
    "                            pass\n",
    "                except:\n",
    "                    for k in ['{\"repo\"'+l for l in line.split('{\"repo\"')[1:]]:\n",
    "                        jsonline=json.loads(unicode(k, errors='ignore').strip())\n",
    "                        last_event=jsonline\n",
    "                        if jsonline.get(\"type\") == \"PushEvent\":\n",
    "                            try:\n",
    "                                user, project, date=parse(jsonline)\n",
    "                                query = 'INSERT OR REPLACE INTO push VALUES (?,?,?)'\n",
    "                                c.execute(query, (project, date, user))\n",
    "                                i = i+1\n",
    "                            except: #repoid or user id missing\n",
    "                                pass\n",
    "        print \"Done file \" + filename + \". Found and inserted \" + str(i) + \" valid PushEvents.\"\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pull_request_merged():\n",
    "    # Create table\n",
    "    conn = sqlite3.connect(dbpath + dbname)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE IF EXISTS pull_request_merged''')\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS pull_request_merged\n",
    "                 (pulrrequestid INTEGER PRIMARY KEY, repositoryid INTEGER, userlogin TEXT, merged_at DATE)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    filenames=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_\") and i.endswith('json')])\n",
    "    for filename in filenames:\n",
    "        conn = sqlite3.connect(dbpath + dbname)\n",
    "        c = conn.cursor()\n",
    "        i = 0\n",
    "        with open(filename) as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                try: \n",
    "                    jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "                    #print jsonline.keys()\n",
    "                    if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['pull_request'].get('merged')==True:\n",
    "                        try:\n",
    "                            pullreq_id, repoid, user, merged_at=parse_pullrequest_merged(jsonline)\n",
    "                            query = 'INSERT INTO pull_request_merged VALUES (?,?,?,?)'\n",
    "                            c.execute(query, (pullreq_id, repoid, user, merged_at))\n",
    "                            i = i+1\n",
    "                        except: #repoid or user id missing\n",
    "                            pass\n",
    "                except:\n",
    "                    for k in ['{\"repo\"'+l for l in line.split('{\"repo\"')[1:]]:\n",
    "                        jsonline=json.loads(unicode(k, errors='ignore').strip())\n",
    "                        last_event=jsonline\n",
    "                        if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['pull_request'].get('merged')==True:\n",
    "                            try:\n",
    "                                pullreq_id, repoid, user, merged_at =parse_pullrequest_merged(jsonline)\n",
    "                                query = 'INSERT INTO pull_request_merged VALUES (?,?,?,?)'\n",
    "                                c.execute(query, (pullreq_id, repoid, user, merged_at))\n",
    "                                i = i+1\n",
    "                            except: #repoid or user id missing\n",
    "                                pass\n",
    "        print \"Done file \" + filename + \". Found and inserted \" + str(i) + \" valid PullRequestMerged.\"\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_pull_request_opened():\n",
    "    # Create table\n",
    "    conn = sqlite3.connect(dbpath + dbname)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE IF EXISTS pull_request_opened''')\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS pull_request_opened\n",
    "                 (pulrrequestid INTEGER PRIMARY KEY, repositoryid INTEGER, userlogin TEXT, opened_at DATE)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    filenames=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_\") and i.endswith('json')])\n",
    "    for filename in filenames:\n",
    "        conn = sqlite3.connect(dbpath + dbname)\n",
    "        c = conn.cursor()\n",
    "        i = 0\n",
    "        with open(filename) as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                try: \n",
    "                    jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "                    #print jsonline.keys()\n",
    "                    if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['action']=='opened':\n",
    "                        try:\n",
    "                            pullreq_id, repoid, user, opened_at=parse_pullrequest_opened(jsonline)\n",
    "                            query = 'INSERT INTO pull_request_opened VALUES (?,?,?,?)'\n",
    "                            c.execute(query, (pullreq_id, repoid, user, opened_at))\n",
    "                            i = i+1\n",
    "                        except: #repoid or user id missing\n",
    "                            pass\n",
    "                except:\n",
    "                    for k in ['{\"repo\"'+l for l in line.split('{\"repo\"')[1:]]:\n",
    "                        jsonline=json.loads(unicode(k, errors='ignore').strip())\n",
    "                        last_event=jsonline\n",
    "                        if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['action']=='opened':\n",
    "                            try:\n",
    "                                pullreq_id, repoid, user, opened_at =parse_pullrequest_opened(jsonline)\n",
    "                                query = 'INSERT INTO pull_request_opened VALUES (?,?,?,?)'\n",
    "                                c.execute(query, (pullreq_id, repoid, user, opened_at))\n",
    "                                i = i+1\n",
    "                            except: #repoid or user id missing\n",
    "                                pass\n",
    "        print \"Done file \" + filename + \". Found and inserted \" + str(i) + \" valid PullRequestOpened.\"\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_pull_request_closed():\n",
    "    # Create table\n",
    "    conn = sqlite3.connect(dbpath + dbname)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE IF EXISTS pull_request_closed''')\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS pull_request_closed\n",
    "                 (pulrrequestid INTEGER PRIMARY KEY, repositoryid INTEGER, userlogin TEXT, closed_at DATE)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    filenames=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_\") and i.endswith('json')])\n",
    "    for filename in filenames:\n",
    "        conn = sqlite3.connect(dbpath + dbname)\n",
    "        c = conn.cursor()\n",
    "        i = 0\n",
    "        with open(filename) as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                try: \n",
    "                    jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "                    #print jsonline.keys()\n",
    "                    if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['action']=='closed':\n",
    "                        try:\n",
    "                            pullreq_id, repoid, user, closed_at=parse_pullrequest_closed(jsonline)\n",
    "                            query = 'INSERT INTO pull_request_closed VALUES (?,?,?,?)'\n",
    "                            c.execute(query, (pullreq_id, repoid, user, closed_at))\n",
    "                            i = i+1\n",
    "                        except: #repoid or user id missing\n",
    "                            pass\n",
    "                except:\n",
    "                    for k in ['{\"repo\"'+l for l in line.split('{\"repo\"')[1:]]:\n",
    "                        jsonline=json.loads(unicode(k, errors='ignore').strip())\n",
    "                        last_event=jsonline\n",
    "                        if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['action']=='closed':\n",
    "                            try:\n",
    "                                pullreq_id, repoid, user, closed_at =parse_pullrequest_closed(jsonline)\n",
    "                                query = 'INSERT INTO pull_request_closed VALUES (?,?,?,?)'\n",
    "                                c.execute(query, (pullreq_id, repoid, user, closed_at))\n",
    "                                i = i+1\n",
    "                            except: #repoid or user id missing\n",
    "                                pass\n",
    "        print \"Done file \" + filename + \". Found and inserted \" + str(i) + \" valid PullRequestClosed.\"\n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done file githubarchive_2011-02-12.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-02-13.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-02-14.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-03-14.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-03-15.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-03-28.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-03-30.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-04-06.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2011-04-08.json. Found and inserted 0 valid PullRequestMerged.\n",
      "Done file githubarchive_2013-02-27.json. Found and inserted 0 valid PullRequestMerged.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#TEST PARSER\\nfs=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_2015\") and i.endswith(\\'json\\')])\\nfor f in fs:\\n    with open(f) as jsonfile:\\n        for line in jsonfile:\\n            jsonline = json.loads(unicode(line, errors=\\'ignore\\').strip())\\n            if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline[\\'payload\\'][\\'pull_request\\'][\\'merged\\']==True:\\n                x=jsonline\\n                parse_pullrequest_merged(jsonline)\\n                alma'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#TEST PARSER\n",
    "fs=sorted([i for i in os.listdir(datapath) if i.startswith(\"githubarchive_2015\") and i.endswith('json')])\n",
    "for f in fs:\n",
    "    with open(f) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            jsonline = json.loads(unicode(line, errors='ignore').strip())\n",
    "            if jsonline.get(\"type\") == \"PullRequestEvent\" and jsonline['payload']['pull_request']['merged']==True:\n",
    "                x=jsonline\n",
    "                parse_pullrequest_merged(jsonline)\n",
    "                alma\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO\n",
    "\n",
    "- add files asn a paramter\n",
    "- join pullrequests\n",
    "- write tests\n",
    "- see parsing differences over years\n",
    "- Write these:\n",
    "    - build_fork()\n",
    "    - build_star()\n",
    "    - build_memberadd()\n",
    "    - build_membership()\n",
    "    - build_follow()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
